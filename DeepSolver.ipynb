{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavAK74/AI_Speedcubing/blob/main/DeepSolver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdYSFIi_RBKB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICuqPrkucCQx",
        "outputId": "5629cfc2-48e0-40b9-91b8-6234a5e8267f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGBIW56WtLT",
        "outputId": "de33f809-dca6-43da-c8c0-28584440b1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrhE5mtoUeFN",
        "outputId": "fb99d1c3-594c-41f9-c705-10bc1090cc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/rubiks_cube/gym-Rubiks-Cube-master\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym_Rubiks_Cube==0.0.1) (0.25.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from gym_Rubiks_Cube==0.0.1) (2.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_Rubiks_Cube==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_Rubiks_Cube==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym_Rubiks_Cube==0.0.1) (0.0.8)\n",
            "Installing collected packages: gym_Rubiks_Cube\n",
            "  Running setup.py develop for gym_Rubiks_Cube\n",
            "Successfully installed gym_Rubiks_Cube-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -e '/content/drive/MyDrive/rubiks_cube/gym-Rubiks-Cube-master'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcN1kd-TcMKT",
        "outputId": "17553df3-5320-4d51-d89f-660b441573c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current sys.path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/drive/My Drive/rubiks_cube/gym-Rubiks-Cube-master', '/content/drive/My Drive/rubiks_cube/gym-Rubiks-Cube-master/gym_Rubiks_Cube/envs']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment RubiksCube-v0\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/rubiks_cube/gym-Rubiks-Cube-master')\n",
        "sys.path.append('/content/drive/My Drive/rubiks_cube/gym-Rubiks-Cube-master/gym_Rubiks_Cube/envs')\n",
        "print('Current sys.path:', sys.path)\n",
        "\n",
        "from gym_Rubiks_Cube.envs.rubiks_cube_env import RubiksCubeEnv\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id='RubiksCube-v0',\n",
        "    entry_point='gym_Rubiks_Cube.envs.rubiks_cube_env:RubiksCubeEnv',\n",
        "    max_episode_steps=1000  # Set a limit on steps per episode\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFkQ0nXey8E6",
        "outputId": "f5a01129-19a2-410e-b0fc-4cb1b7fc9f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cube State:\n",
            "[1 5 1 1 5 2 2 2 2 5 5 0 5 2 2 5 0 0 4 0 0 3 1 1 3 1 1 2 2 4 0 0 4 5 5 3 4\n",
            " 3 3 4 3 3 5 0 2 1 1 0 4 4 3 4 4 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be uint8, actual type: int64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('RubiksCube-v0')\n",
        "# Before training the model, let's visualize the initial state of the cube to ensure it's being represented properly.\n",
        "initial_state = env.reset()\n",
        "print(\"Initial Cube State:\")\n",
        "print(initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6KrPUM_egyd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kj3VBCtbcgn"
      },
      "outputs": [],
      "source": [
        "def enhanced_render(self, mode='human', close=False):\n",
        "    if close:\n",
        "        return\n",
        "\n",
        "    # Color mapping for tiles\n",
        "    color_map = {\n",
        "        'R': 'red',\n",
        "        'O': 'orange',\n",
        "        'Y': 'yellow',\n",
        "        'G': 'green',\n",
        "        'B': 'blue',\n",
        "        'W': 'white',\n",
        "    }\n",
        "\n",
        "    # Cube faces\n",
        "    faces = {\n",
        "        'front': self.ncube.front,\n",
        "        'back': self.ncube.back,\n",
        "        'left': self.ncube.left,\n",
        "        'right': self.ncube.right,\n",
        "        'up': self.ncube.up,\n",
        "        'down': self.ncube.down,\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "    # Define face positions\n",
        "    face_positions = {\n",
        "        'up': (1, 3),\n",
        "        'left': (0, 2),\n",
        "        'front': (1, 2),\n",
        "        'right': (2, 2),\n",
        "        'back': (3, 2),\n",
        "        'down': (1, 1),\n",
        "    }\n",
        "\n",
        "    face_size = self.orderNum\n",
        "    for face, (x_offset, y_offset) in face_positions.items():\n",
        "        for i in range(face_size):\n",
        "            for j in range(face_size):\n",
        "                color = color_map[faces[face][i][j].strip()]\n",
        "                rect = Rectangle((x_offset * face_size + j, y_offset * face_size + (face_size - i - 1)),\n",
        "                                 1, 1, facecolor=color, edgecolor='black')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "    ax.set_xlim(0, 4 * face_size)\n",
        "    ax.set_ylim(0, 4 * face_size)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')  # Hide the axis\n",
        "    plt.show()\n",
        "\n",
        "# Dynamically bind the new render method to the RubiksCubeEnv class\n",
        "RubiksCubeEnv.render = enhanced_render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "sbhfl-XZe6rL",
        "outputId": "43441f3f-3478-4ff4-a0d6-8845a2c6aee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZmQ1wvFqMlE"
      },
      "outputs": [],
      "source": [
        "from gym_Rubiks_Cube.envs.rubiks_cube_env import RubiksCubeEnv\n",
        "from gym_Rubiks_Cube.envs.cube import Cube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQKQnAYuqRMb"
      },
      "outputs": [],
      "source": [
        "from gym_Rubiks_Cube.envs.rubiks_cube_env import tileDict, actionList\n",
        "\n",
        "# Extend RubiksCubeEnv with new reward function and state representation\n",
        "def calculate_progress_reward(self):\n",
        "    # Base reward for solved faces\n",
        "    correct_faces = sum(\n",
        "        all(tile == row[0] for row in face for tile in row)\n",
        "        for face in [self.ncube.front, self.ncube.back, self.ncube.left,\n",
        "                     self.ncube.right, self.ncube.up, self.ncube.down]\n",
        "    )\n",
        "    face_reward = correct_faces / 6  # Fraction of solved faces\n",
        "\n",
        "    # Reward for partially aligned tiles\n",
        "    aligned_tiles = sum(\n",
        "        tile == face[0][0] for face in [self.ncube.front, self.ncube.back, self.ncube.left,\n",
        "                                        self.ncube.right, self.ncube.up, self.ncube.down]\n",
        "        for row in face for tile in row\n",
        "    )\n",
        "    alignment_reward = aligned_tiles / 54  # Fraction of aligned tiles\n",
        "\n",
        "    return face_reward * 2 + alignment_reward  # Weight solved faces higher\n",
        "\n",
        "def step_with_progress_reward(self, action):\n",
        "    # Apply the action to the cube\n",
        "    self.action_log.append(action)\n",
        "    self.ncube.minimalInterpreter(actionList[action])\n",
        "    self.state = self.getstate()\n",
        "    self.step_count += 1\n",
        "\n",
        "    # Calculate reward\n",
        "    reward = self.calculate_progress_reward()\n",
        "    done = False\n",
        "    info = {}  # Additional info dictionary (can include debugging or analysis data)\n",
        "\n",
        "    # Check if the cube is solved\n",
        "    if self.ncube.isSolved():\n",
        "        reward += 1000  # Add a bonus reward for solving\n",
        "        done = True\n",
        "\n",
        "    # End the episode if the maximum steps are reached\n",
        "    if self.step_count > 50:\n",
        "        done = True\n",
        "\n",
        "    return self.state, reward, done, info\n",
        "\n",
        "def getstate_one_hot(self):\n",
        "    # Generate a one-hot encoded state representation\n",
        "    one_hot_state = np.zeros((6 * self.orderNum * self.orderNum, 6))  # 6 faces, 6 colors\n",
        "    tile_vector = self.ncube.constructVectorState()\n",
        "    for idx, tile in enumerate(tile_vector):\n",
        "        one_hot_state[idx][tileDict[tile]] = 1\n",
        "    return one_hot_state.flatten()  # Flatten for neural network input\n",
        "\n",
        "\n",
        "\n",
        "# Dynamically override methods in RubiksCubeEnv\n",
        "RubiksCubeEnv.calculate_progress_reward = calculate_progress_reward\n",
        "RubiksCubeEnv.step = step_with_progress_reward\n",
        "RubiksCubeEnv.getstate = getstate_one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJfVnybNmvJm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XYm7pPysi5q"
      },
      "outputs": [],
      "source": [
        "def reset_model_weights(model):\n",
        "    for layer in model.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLlbi5huWvyM",
        "outputId": "116f5938-1982-41e8-ca5e-a00570fc8e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir=\"runs/rubiks_cube_dqn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZXggokWfUwQ",
        "outputId": "024a2457-8c1f-4310-c17b-d88af270d87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from torch import nn, optim\n",
        "from gym_Rubiks_Cube.envs.rubiks_cube_env import RubiksCubeEnv\n",
        "\n",
        "# Define model and training parameters\n",
        "STATE_SIZE = 6 * 6 * 9\n",
        "ACTION_SIZE = 12\n",
        "GAMMA = 0.99\n",
        "LR = 2e-4\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_CAPACITY = 20000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.1\n",
        "EPSILON_DECAY = 0.995\n",
        "TARGET_UPDATE_FREQ = 10\n",
        "EPISODES = 5000\n",
        "MAX_STEPS = 50\n",
        "\n",
        "# Initialize Rubik's Cube environment\n",
        "env = RubiksCubeEnv()\n",
        "env.setScramble(low=1, high=3)\n",
        "env.reset()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define Noisy Linear Layer\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
        "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
        "\n",
        "        self.sigma_init = sigma_init\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        bound = 1 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-bound, bound)\n",
        "        self.bias_mu.data.uniform_(-bound, bound)\n",
        "        self.weight_sigma.data.fill_(self.sigma_init)\n",
        "        self.bias_sigma.data.fill_(self.sigma_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            self.weight_epsilon.normal_()\n",
        "            self.bias_epsilon.normal_()\n",
        "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
        "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
        "        else:\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "        return torch.nn.functional.linear(input, weight, bias)\n",
        "\n",
        "    def reset_noise(self):\n",
        "        self.weight_epsilon.normal_()\n",
        "        self.bias_epsilon.normal_()\n",
        "\n",
        "# Define DQN Model\n",
        "class NoisyDQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(NoisyDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 256)\n",
        "        self.noisy1 = NoisyLinear(256, 256)\n",
        "        self.noisy2 = NoisyLinear(256, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.noisy1(x))\n",
        "        return self.noisy2(x)\n",
        "\n",
        "    def reset_noise(self):\n",
        "        self.noisy1.reset_noise()\n",
        "        self.noisy2.reset_noise()\n",
        "\n",
        "# Initialize policy and target networks\n",
        "policy_net = NoisyDQN(STATE_SIZE, ACTION_SIZE).to(device)\n",
        "target_net = NoisyDQN(STATE_SIZE, ACTION_SIZE).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "# Define replay buffer\n",
        "# Replay Buffer with corrected sample method\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_priority = self.priorities.max() if len(self.buffer) > 0 else 1.0\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.priorities[self.position] = max_priority\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == 0:\n",
        "            raise ValueError(\"The replay buffer is empty!\")\n",
        "        priorities = self.priorities[:len(self.buffer)]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        # Unpack the samples\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "\n",
        "        # Convert lists of NumPy arrays to a single NumPy array\n",
        "        states = np.stack(states)\n",
        "        next_states = np.stack(next_states)\n",
        "\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return (\n",
        "            torch.tensor(states, dtype=torch.float32, device=device),\n",
        "            torch.tensor(actions, dtype=torch.long, device=device),\n",
        "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
        "            torch.tensor(next_states, dtype=torch.float32, device=device),\n",
        "            torch.tensor(dones, dtype=torch.float32, device=device),\n",
        "            torch.tensor(weights, dtype=torch.float32, device=device),\n",
        "            indices,\n",
        "        )\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "replay_buffer = PrioritizedReplayBuffer(BUFFER_CAPACITY)\n",
        "\n",
        "def dynamic_epsilon(episode, base_epsilon=1.0, min_epsilon=0.1, decay_rate=0.995, success_rate=None):\n",
        "    \"\"\"\n",
        "    Adjusts epsilon dynamically based on the episode number or success rate.\n",
        "\n",
        "    Args:\n",
        "    - episode: Current episode number.\n",
        "    - base_epsilon: Starting epsilon value.\n",
        "    - min_epsilon: Minimum value of epsilon.\n",
        "    - decay_rate: Standard decay rate for epsilon.\n",
        "    - success_rate: Optional, current success rate of the agent.\n",
        "\n",
        "    Returns:\n",
        "    - epsilon: Adjusted exploration rate.\n",
        "    \"\"\"\n",
        "    # Standard decay based on episode number\n",
        "    epsilon = max(min_epsilon, base_epsilon * (decay_rate ** episode))\n",
        "\n",
        "    # Adjust epsilon based on success rate\n",
        "    if success_rate is not None:\n",
        "        if success_rate > 0.7:  # Lower exploration if success rate is high\n",
        "            epsilon = max(min_epsilon, epsilon * 0.9)\n",
        "        elif success_rate < 0.4:  # Increase exploration if success rate is low\n",
        "            epsilon = min(1.0, epsilon * 1.1)\n",
        "\n",
        "    return epsilon\n",
        "\n",
        "def epsilon_greedy_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    with torch.no_grad():\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        q_values = policy_net(state)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model_with_curriculum(\n",
        "    episodes, epsilon_start, epsilon_end, target_success_rate=0.8, max_scramble_depth=20\n",
        "):\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    rolling_success = []\n",
        "    epsilon = epsilon_start\n",
        "    depth_range = [1, 3]  # Start with a scramble depth range of 1 to 3\n",
        "    last_difficulty_increase = -10\n",
        "\n",
        "    def get_success_threshold(depth):\n",
        "        if depth <= 5:\n",
        "            return 0.7  # 70% for depths 1–5\n",
        "        elif depth <= 8:\n",
        "            return 0.55  # 60% for depths 6–8\n",
        "        elif depth <= 11:\n",
        "            return 0.5 # 45% for depths 9–11\n",
        "        elif depth <= 15:\n",
        "            return 0.4  # 40% for depths 12–14\n",
        "        else:\n",
        "            return 0.3  # 40% for depths 16–20\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        # Update scramble depth dynamically\n",
        "        scramble_depth = random.randint(*depth_range)\n",
        "        env.setScramble(low=scramble_depth, high=scramble_depth)  # Set scramble depth\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device) / 5\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(MAX_STEPS):\n",
        "            action = epsilon_greedy_action(state, epsilon)\n",
        "            policy_net.reset_noise()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device) / 5\n",
        "            replay_buffer.push(state.cpu() / 5, action, reward, next_state.cpu() / 5, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train the model if sufficient samples are available\n",
        "            if len(replay_buffer) >= BATCH_SIZE:\n",
        "                states, actions, rewards, next_states, dones, weights, indices = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "                with torch.no_grad():\n",
        "                    max_next_q_values = target_net(next_states).max(1)[0]\n",
        "                    target_q_values = rewards + (1 - dones) * GAMMA * max_next_q_values\n",
        "\n",
        "                loss = (weights * (q_values - target_q_values).pow(2)).mean()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=10)\n",
        "                optimizer.step()\n",
        "\n",
        "                td_errors = (q_values - target_q_values).detach().cpu().numpy()\n",
        "                replay_buffer.update_priorities(indices, np.abs(td_errors) + 1e-6)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Track rolling success rate\n",
        "        solved = 1 if env.ncube.isSolved() else 0\n",
        "        '''if (random.randint(1,2) < 1.1 and depth_range[1] >= 7):\n",
        "          solved = 1'''\n",
        "        rolling_success.append(solved)\n",
        "        if len(rolling_success) > 100:\n",
        "            rolling_success.pop(0)\n",
        "        avg_success_rate = sum(rolling_success) / len(rolling_success)\n",
        "\n",
        "        epsilon = dynamic_epsilon(episode, base_epsilon=EPSILON_START, min_epsilon=EPSILON_END, decay_rate=EPSILON_DECAY, success_rate=avg_success_rate)\n",
        "\n",
        "        # Adjust depth range based on success rate\n",
        "        success_threshold = get_success_threshold(depth_range[1])\n",
        "        if avg_success_rate >= success_threshold and depth_range[1] < max_scramble_depth and episode - last_difficulty_increase >= 200:\n",
        "            #depth_range[0] += 1\n",
        "            last_difficulty_increase = episode\n",
        "            depth_range[1] += 1\n",
        "            print(f\"Difficulty increased to {depth_range}\")\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode > 0 and episode % TARGET_UPDATE_FREQ == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            target_net.reset_noise()\n",
        "\n",
        "        # Log progress\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}: Total Reward: {total_reward}, Avg Success Rate: {avg_success_rate:.2%}, \"\n",
        "                  f\"Scramble Depth: {depth_range[1]}\")\n",
        "\n",
        "        # Save models if success rate is achieved at max depth\n",
        "        if depth_range[1] == max_scramble_depth and avg_success_rate >= target_success_rate:\n",
        "            print(f\"Success rate reached at scramble depth {max_scramble_depth}. Saving model...\")\n",
        "            torch.save(policy_net.state_dict(), f\"policy_net_scramble_{max_scramble_depth}.pth\")\n",
        "            torch.save(target_net.state_dict(), f\"target_net_scramble_{max_scramble_depth}.pth\")\n",
        "            break\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_model_with_curriculum(\n",
        "    episodes=5000,\n",
        "    epsilon_start=EPSILON_START,\n",
        "    epsilon_end=EPSILON_END,\n",
        "    target_success_rate=0.5,\n",
        "    max_scramble_depth=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcI44cfdyNXI",
        "outputId": "049cbf08-7a2a-46b6-b654-735436a24acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Episode 0: Total Reward: 12.57407407407407, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 10: Total Reward: 15.12962962962963, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 20: Total Reward: 12.259259259259256, Avg Success Rate: 0.00%, Scramble Depth: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8925b7fb06c1>:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 30: Total Reward: 17.240740740740737, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 40: Total Reward: 17.24074074074074, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 50: Total Reward: 12.129629629629626, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 60: Total Reward: 15.500000000000004, Avg Success Rate: 0.00%, Scramble Depth: 3\n",
            "Episode 70: Total Reward: 1008.5555555555555, Avg Success Rate: 1.41%, Scramble Depth: 3\n",
            "Episode 80: Total Reward: 12.759259259259254, Avg Success Rate: 1.23%, Scramble Depth: 3\n",
            "Episode 90: Total Reward: 17.055555555555554, Avg Success Rate: 2.20%, Scramble Depth: 3\n",
            "Episode 100: Total Reward: 15.148148148148143, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 110: Total Reward: 14.648148148148145, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 120: Total Reward: 14.83333333333333, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 130: Total Reward: 14.574074074074076, Avg Success Rate: 5.00%, Scramble Depth: 3\n",
            "Episode 140: Total Reward: 15.981481481481483, Avg Success Rate: 5.00%, Scramble Depth: 3\n",
            "Episode 150: Total Reward: 14.425925925925924, Avg Success Rate: 5.00%, Scramble Depth: 3\n",
            "Episode 160: Total Reward: 12.481481481481483, Avg Success Rate: 5.00%, Scramble Depth: 3\n",
            "Episode 170: Total Reward: 18.12962962962963, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 180: Total Reward: 14.611111111111116, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 190: Total Reward: 17.888888888888893, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 200: Total Reward: 23.592592592592588, Avg Success Rate: 2.00%, Scramble Depth: 3\n",
            "Episode 210: Total Reward: 16.759259259259263, Avg Success Rate: 2.00%, Scramble Depth: 3\n",
            "Episode 220: Total Reward: 1003.0, Avg Success Rate: 5.00%, Scramble Depth: 3\n",
            "Episode 230: Total Reward: 18.999999999999996, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 240: Total Reward: 16.24074074074074, Avg Success Rate: 4.00%, Scramble Depth: 3\n",
            "Episode 250: Total Reward: 14.148148148148147, Avg Success Rate: 6.00%, Scramble Depth: 3\n",
            "Episode 260: Total Reward: 32.85185185185184, Avg Success Rate: 7.00%, Scramble Depth: 3\n",
            "Episode 270: Total Reward: 14.351851851851853, Avg Success Rate: 10.00%, Scramble Depth: 3\n",
            "Episode 280: Total Reward: 20.66666666666666, Avg Success Rate: 13.00%, Scramble Depth: 3\n",
            "Episode 290: Total Reward: 18.481481481481474, Avg Success Rate: 14.00%, Scramble Depth: 3\n",
            "Episode 300: Total Reward: 1005.7777777777778, Avg Success Rate: 17.00%, Scramble Depth: 3\n",
            "Episode 310: Total Reward: 1016.9629629629629, Avg Success Rate: 20.00%, Scramble Depth: 3\n",
            "Episode 320: Total Reward: 22.111111111111114, Avg Success Rate: 19.00%, Scramble Depth: 3\n",
            "Episode 330: Total Reward: 1014.7407407407408, Avg Success Rate: 22.00%, Scramble Depth: 3\n",
            "Episode 340: Total Reward: 44.55555555555556, Avg Success Rate: 24.00%, Scramble Depth: 3\n",
            "Episode 350: Total Reward: 1005.7777777777778, Avg Success Rate: 27.00%, Scramble Depth: 3\n",
            "Episode 360: Total Reward: 27.01851851851851, Avg Success Rate: 30.00%, Scramble Depth: 3\n",
            "Episode 370: Total Reward: 13.148148148148147, Avg Success Rate: 31.00%, Scramble Depth: 3\n",
            "Episode 380: Total Reward: 1014.6666666666666, Avg Success Rate: 33.00%, Scramble Depth: 3\n",
            "Episode 390: Total Reward: 1003.0, Avg Success Rate: 38.00%, Scramble Depth: 3\n",
            "Episode 400: Total Reward: 1008.1111111111111, Avg Success Rate: 43.00%, Scramble Depth: 3\n",
            "Episode 410: Total Reward: 1014.1111111111111, Avg Success Rate: 46.00%, Scramble Depth: 3\n",
            "Episode 420: Total Reward: 1003.0, Avg Success Rate: 50.00%, Scramble Depth: 3\n",
            "Episode 430: Total Reward: 19.96296296296296, Avg Success Rate: 51.00%, Scramble Depth: 3\n",
            "Episode 440: Total Reward: 21.03703703703702, Avg Success Rate: 56.00%, Scramble Depth: 3\n",
            "Episode 450: Total Reward: 22.222222222222225, Avg Success Rate: 59.00%, Scramble Depth: 3\n",
            "Episode 460: Total Reward: 1003.0, Avg Success Rate: 60.00%, Scramble Depth: 3\n",
            "Episode 470: Total Reward: 20.055555555555554, Avg Success Rate: 62.00%, Scramble Depth: 3\n",
            "Episode 480: Total Reward: 21.38888888888887, Avg Success Rate: 65.00%, Scramble Depth: 3\n",
            "Episode 490: Total Reward: 15.96296296296296, Avg Success Rate: 65.00%, Scramble Depth: 3\n",
            "Episode 500: Total Reward: 1011.4444444444445, Avg Success Rate: 61.00%, Scramble Depth: 3\n",
            "Episode 510: Total Reward: 1003.0, Avg Success Rate: 62.00%, Scramble Depth: 3\n",
            "Episode 520: Total Reward: 29.166666666666654, Avg Success Rate: 65.00%, Scramble Depth: 3\n",
            "Episode 530: Total Reward: 1005.0, Avg Success Rate: 67.00%, Scramble Depth: 3\n",
            "Episode 540: Total Reward: 1003.0, Avg Success Rate: 68.00%, Scramble Depth: 3\n",
            "Episode 550: Total Reward: 21.25925925925925, Avg Success Rate: 66.00%, Scramble Depth: 3\n",
            "Episode 560: Total Reward: 26.40740740740742, Avg Success Rate: 68.00%, Scramble Depth: 3\n",
            "Difficulty increased to [1, 4]\n",
            "Episode 570: Total Reward: 1003.0, Avg Success Rate: 71.00%, Scramble Depth: 4\n",
            "Episode 580: Total Reward: 1003.0, Avg Success Rate: 70.00%, Scramble Depth: 4\n",
            "Episode 590: Total Reward: 16.796296296296298, Avg Success Rate: 70.00%, Scramble Depth: 4\n",
            "Episode 600: Total Reward: 21.592592592592585, Avg Success Rate: 73.00%, Scramble Depth: 4\n",
            "Episode 610: Total Reward: 1003.0, Avg Success Rate: 75.00%, Scramble Depth: 4\n",
            "Episode 620: Total Reward: 19.981481481481477, Avg Success Rate: 73.00%, Scramble Depth: 4\n",
            "Episode 630: Total Reward: 22.648148148148167, Avg Success Rate: 72.00%, Scramble Depth: 4\n",
            "Episode 640: Total Reward: 27.833333333333325, Avg Success Rate: 69.00%, Scramble Depth: 4\n",
            "Episode 650: Total Reward: 1011.8148148148148, Avg Success Rate: 69.00%, Scramble Depth: 4\n",
            "Episode 660: Total Reward: 1010.8888888888889, Avg Success Rate: 69.00%, Scramble Depth: 4\n",
            "Episode 670: Total Reward: 20.12962962962963, Avg Success Rate: 64.00%, Scramble Depth: 4\n",
            "Episode 680: Total Reward: 1006.5370370370371, Avg Success Rate: 65.00%, Scramble Depth: 4\n",
            "Episode 690: Total Reward: 1006.0370370370371, Avg Success Rate: 65.00%, Scramble Depth: 4\n",
            "Episode 700: Total Reward: 1003.0, Avg Success Rate: 65.00%, Scramble Depth: 4\n",
            "Episode 710: Total Reward: 23.40740740740741, Avg Success Rate: 61.00%, Scramble Depth: 4\n",
            "Episode 720: Total Reward: 1014.3888888888889, Avg Success Rate: 62.00%, Scramble Depth: 4\n",
            "Episode 730: Total Reward: 16.0925925925926, Avg Success Rate: 63.00%, Scramble Depth: 4\n",
            "Episode 740: Total Reward: 1004.3888888888889, Avg Success Rate: 65.00%, Scramble Depth: 4\n",
            "Episode 750: Total Reward: 1005.2962962962963, Avg Success Rate: 68.00%, Scramble Depth: 4\n",
            "Episode 760: Total Reward: 1006.5925925925926, Avg Success Rate: 66.00%, Scramble Depth: 4\n",
            "Episode 770: Total Reward: 1008.7037037037037, Avg Success Rate: 69.00%, Scramble Depth: 4\n",
            "Difficulty increased to [1, 5]\n",
            "Episode 780: Total Reward: 18.31481481481482, Avg Success Rate: 67.00%, Scramble Depth: 5\n",
            "Episode 790: Total Reward: 19.24074074074074, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 800: Total Reward: 16.277777777777782, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 810: Total Reward: 21.518518518518523, Avg Success Rate: 66.00%, Scramble Depth: 5\n",
            "Episode 820: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 830: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 840: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 850: Total Reward: 25.33333333333332, Avg Success Rate: 61.00%, Scramble Depth: 5\n",
            "Episode 860: Total Reward: 20.18518518518518, Avg Success Rate: 60.00%, Scramble Depth: 5\n",
            "Episode 870: Total Reward: 1003.0, Avg Success Rate: 62.00%, Scramble Depth: 5\n",
            "Episode 880: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 890: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 5\n",
            "Episode 900: Total Reward: 1011.0, Avg Success Rate: 63.00%, Scramble Depth: 5\n",
            "Episode 910: Total Reward: 29.83333333333332, Avg Success Rate: 61.00%, Scramble Depth: 5\n",
            "Episode 920: Total Reward: 15.685185185185185, Avg Success Rate: 61.00%, Scramble Depth: 5\n",
            "Episode 930: Total Reward: 1003.0, Avg Success Rate: 62.00%, Scramble Depth: 5\n",
            "Episode 940: Total Reward: 1005.7777777777778, Avg Success Rate: 59.00%, Scramble Depth: 5\n",
            "Episode 950: Total Reward: 1008.8703703703703, Avg Success Rate: 60.00%, Scramble Depth: 5\n",
            "Episode 960: Total Reward: 1003.0, Avg Success Rate: 62.00%, Scramble Depth: 5\n",
            "Episode 970: Total Reward: 1003.0, Avg Success Rate: 58.00%, Scramble Depth: 5\n",
            "Episode 980: Total Reward: 17.203703703703706, Avg Success Rate: 56.00%, Scramble Depth: 5\n",
            "Episode 990: Total Reward: 1006.6111111111111, Avg Success Rate: 57.00%, Scramble Depth: 5\n",
            "Episode 1000: Total Reward: 18.185185185185187, Avg Success Rate: 55.00%, Scramble Depth: 5\n",
            "Episode 1010: Total Reward: 1009.1296296296297, Avg Success Rate: 58.00%, Scramble Depth: 5\n",
            "Episode 1020: Total Reward: 1003.0, Avg Success Rate: 59.00%, Scramble Depth: 5\n",
            "Episode 1030: Total Reward: 14.407407407407408, Avg Success Rate: 57.00%, Scramble Depth: 5\n",
            "Episode 1040: Total Reward: 1007.074074074074, Avg Success Rate: 62.00%, Scramble Depth: 5\n",
            "Episode 1050: Total Reward: 21.62962962962962, Avg Success Rate: 61.00%, Scramble Depth: 5\n",
            "Episode 1060: Total Reward: 1003.0, Avg Success Rate: 62.00%, Scramble Depth: 5\n",
            "Episode 1070: Total Reward: 1003.0, Avg Success Rate: 67.00%, Scramble Depth: 5\n",
            "Episode 1080: Total Reward: 14.648148148148147, Avg Success Rate: 68.00%, Scramble Depth: 5\n",
            "Episode 1090: Total Reward: 15.203703703703702, Avg Success Rate: 68.00%, Scramble Depth: 5\n",
            "Difficulty increased to [1, 6]\n",
            "Episode 1100: Total Reward: 1005.5555555555555, Avg Success Rate: 69.00%, Scramble Depth: 6\n",
            "Episode 1110: Total Reward: 26.18518518518518, Avg Success Rate: 66.00%, Scramble Depth: 6\n",
            "Episode 1120: Total Reward: 1009.0, Avg Success Rate: 65.00%, Scramble Depth: 6\n",
            "Episode 1130: Total Reward: 1005.6111111111111, Avg Success Rate: 66.00%, Scramble Depth: 6\n",
            "Episode 1140: Total Reward: 1003.0, Avg Success Rate: 64.00%, Scramble Depth: 6\n",
            "Episode 1150: Total Reward: 1006.3148148148148, Avg Success Rate: 65.00%, Scramble Depth: 6\n",
            "Episode 1160: Total Reward: 1006.5, Avg Success Rate: 66.00%, Scramble Depth: 6\n",
            "Episode 1170: Total Reward: 22.203703703703706, Avg Success Rate: 61.00%, Scramble Depth: 6\n",
            "Episode 1180: Total Reward: 15.277777777777777, Avg Success Rate: 61.00%, Scramble Depth: 6\n",
            "Episode 1190: Total Reward: 13.94444444444445, Avg Success Rate: 60.00%, Scramble Depth: 6\n",
            "Episode 1200: Total Reward: 1006.3148148148148, Avg Success Rate: 62.00%, Scramble Depth: 6\n",
            "Episode 1210: Total Reward: 22.388888888888875, Avg Success Rate: 63.00%, Scramble Depth: 6\n",
            "Episode 1220: Total Reward: 22.481481481481474, Avg Success Rate: 66.00%, Scramble Depth: 6\n",
            "Episode 1230: Total Reward: 1007.2222222222222, Avg Success Rate: 68.00%, Scramble Depth: 6\n",
            "Episode 1240: Total Reward: 1007.0, Avg Success Rate: 69.00%, Scramble Depth: 6\n",
            "Episode 1250: Total Reward: 1004.2777777777778, Avg Success Rate: 70.00%, Scramble Depth: 6\n",
            "Episode 1260: Total Reward: 1005.0, Avg Success Rate: 70.00%, Scramble Depth: 6\n",
            "Episode 1270: Total Reward: 1003.0, Avg Success Rate: 75.00%, Scramble Depth: 6\n",
            "Episode 1280: Total Reward: 1003.0, Avg Success Rate: 75.00%, Scramble Depth: 6\n",
            "Episode 1290: Total Reward: 15.333333333333329, Avg Success Rate: 77.00%, Scramble Depth: 6\n",
            "Difficulty increased to [1, 7]\n",
            "Episode 1300: Total Reward: 14.851851851851853, Avg Success Rate: 74.00%, Scramble Depth: 7\n",
            "Episode 1310: Total Reward: 1013.9444444444445, Avg Success Rate: 75.00%, Scramble Depth: 7\n",
            "Episode 1320: Total Reward: 14.962962962962973, Avg Success Rate: 72.00%, Scramble Depth: 7\n",
            "Episode 1330: Total Reward: 18.259259259259256, Avg Success Rate: 68.00%, Scramble Depth: 7\n",
            "Episode 1340: Total Reward: 21.314814814814813, Avg Success Rate: 64.00%, Scramble Depth: 7\n",
            "Episode 1350: Total Reward: 1011.1111111111111, Avg Success Rate: 62.00%, Scramble Depth: 7\n",
            "Episode 1360: Total Reward: 10.537037037037036, Avg Success Rate: 59.00%, Scramble Depth: 7\n",
            "Episode 1370: Total Reward: 1005.0, Avg Success Rate: 52.00%, Scramble Depth: 7\n",
            "Episode 1380: Total Reward: 14.962962962962962, Avg Success Rate: 51.00%, Scramble Depth: 7\n",
            "Episode 1390: Total Reward: 13.074074074074076, Avg Success Rate: 51.00%, Scramble Depth: 7\n",
            "Episode 1400: Total Reward: 1005.5370370370371, Avg Success Rate: 54.00%, Scramble Depth: 7\n",
            "Episode 1410: Total Reward: 1005.5555555555555, Avg Success Rate: 52.00%, Scramble Depth: 7\n",
            "Episode 1420: Total Reward: 1003.0, Avg Success Rate: 52.00%, Scramble Depth: 7\n",
            "Episode 1430: Total Reward: 1006.3148148148148, Avg Success Rate: 55.00%, Scramble Depth: 7\n",
            "Episode 1440: Total Reward: 1011.2592592592592, Avg Success Rate: 57.00%, Scramble Depth: 7\n",
            "Episode 1450: Total Reward: 1005.7777777777778, Avg Success Rate: 57.00%, Scramble Depth: 7\n",
            "Episode 1460: Total Reward: 12.407407407407414, Avg Success Rate: 57.00%, Scramble Depth: 7\n",
            "Episode 1470: Total Reward: 14.888888888888884, Avg Success Rate: 58.00%, Scramble Depth: 7\n",
            "Episode 1480: Total Reward: 1005.0, Avg Success Rate: 55.00%, Scramble Depth: 7\n",
            "Episode 1490: Total Reward: 31.240740740740716, Avg Success Rate: 55.00%, Scramble Depth: 7\n",
            "Difficulty increased to [1, 8]\n",
            "Episode 1500: Total Reward: 17.259259259259256, Avg Success Rate: 52.00%, Scramble Depth: 8\n",
            "Episode 1510: Total Reward: 12.66666666666667, Avg Success Rate: 52.00%, Scramble Depth: 8\n",
            "Episode 1520: Total Reward: 1008.074074074074, Avg Success Rate: 51.00%, Scramble Depth: 8\n",
            "Episode 1530: Total Reward: 15.629629629629626, Avg Success Rate: 47.00%, Scramble Depth: 8\n",
            "Episode 1540: Total Reward: 19.444444444444446, Avg Success Rate: 47.00%, Scramble Depth: 8\n",
            "Episode 1550: Total Reward: 12.203703703703706, Avg Success Rate: 42.00%, Scramble Depth: 8\n",
            "Episode 1560: Total Reward: 1003.0, Avg Success Rate: 39.00%, Scramble Depth: 8\n",
            "Episode 1570: Total Reward: 15.462962962962958, Avg Success Rate: 38.00%, Scramble Depth: 8\n",
            "Episode 1580: Total Reward: 26.94444444444445, Avg Success Rate: 40.00%, Scramble Depth: 8\n",
            "Episode 1590: Total Reward: 1006.7592592592592, Avg Success Rate: 36.00%, Scramble Depth: 8\n",
            "Episode 1600: Total Reward: 1007.8888888888889, Avg Success Rate: 37.00%, Scramble Depth: 8\n",
            "Episode 1610: Total Reward: 13.314814814814817, Avg Success Rate: 38.00%, Scramble Depth: 8\n",
            "Episode 1620: Total Reward: 13.629629629629633, Avg Success Rate: 38.00%, Scramble Depth: 8\n",
            "Episode 1630: Total Reward: 12.888888888888888, Avg Success Rate: 42.00%, Scramble Depth: 8\n",
            "Episode 1640: Total Reward: 1011.2222222222222, Avg Success Rate: 42.00%, Scramble Depth: 8\n",
            "Episode 1650: Total Reward: 1006.8888888888889, Avg Success Rate: 47.00%, Scramble Depth: 8\n",
            "Episode 1660: Total Reward: 12.407407407407412, Avg Success Rate: 47.00%, Scramble Depth: 8\n",
            "Episode 1670: Total Reward: 1006.4629629629629, Avg Success Rate: 53.00%, Scramble Depth: 8\n",
            "Episode 1680: Total Reward: 13.425925925925924, Avg Success Rate: 53.00%, Scramble Depth: 8\n",
            "Episode 1690: Total Reward: 14.833333333333336, Avg Success Rate: 58.00%, Scramble Depth: 8\n",
            "Difficulty increased to [1, 9]\n",
            "Episode 1700: Total Reward: 1003.0, Avg Success Rate: 60.00%, Scramble Depth: 9\n",
            "Episode 1710: Total Reward: 17.518518518518523, Avg Success Rate: 59.00%, Scramble Depth: 9\n",
            "Episode 1720: Total Reward: 1008.9444444444445, Avg Success Rate: 60.00%, Scramble Depth: 9\n",
            "Episode 1730: Total Reward: 24.074074074074076, Avg Success Rate: 57.00%, Scramble Depth: 9\n",
            "Episode 1740: Total Reward: 30.555555555555546, Avg Success Rate: 56.00%, Scramble Depth: 9\n",
            "Episode 1750: Total Reward: 25.111111111111118, Avg Success Rate: 53.00%, Scramble Depth: 9\n",
            "Episode 1760: Total Reward: 16.037037037037045, Avg Success Rate: 57.00%, Scramble Depth: 9\n",
            "Episode 1770: Total Reward: 1007.1666666666666, Avg Success Rate: 53.00%, Scramble Depth: 9\n",
            "Episode 1780: Total Reward: 26.129629629629623, Avg Success Rate: 51.00%, Scramble Depth: 9\n",
            "Episode 1790: Total Reward: 1007.9629629629629, Avg Success Rate: 50.00%, Scramble Depth: 9\n",
            "Episode 1800: Total Reward: 40.166666666666664, Avg Success Rate: 46.00%, Scramble Depth: 9\n",
            "Episode 1810: Total Reward: 1003.0, Avg Success Rate: 43.00%, Scramble Depth: 9\n",
            "Episode 1820: Total Reward: 15.888888888888888, Avg Success Rate: 42.00%, Scramble Depth: 9\n",
            "Episode 1830: Total Reward: 1003.0, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1840: Total Reward: 24.925925925925945, Avg Success Rate: 43.00%, Scramble Depth: 9\n",
            "Episode 1850: Total Reward: 13.944444444444445, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1860: Total Reward: 1003.0, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1870: Total Reward: 23.61111111111112, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1880: Total Reward: 20.425925925925934, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1890: Total Reward: 1010.5555555555555, Avg Success Rate: 39.00%, Scramble Depth: 9\n",
            "Episode 1900: Total Reward: 1005.6296296296297, Avg Success Rate: 42.00%, Scramble Depth: 9\n",
            "Episode 1910: Total Reward: 16.407407407407415, Avg Success Rate: 45.00%, Scramble Depth: 9\n",
            "Episode 1920: Total Reward: 11.148148148148143, Avg Success Rate: 46.00%, Scramble Depth: 9\n",
            "Episode 1930: Total Reward: 36.11111111111112, Avg Success Rate: 43.00%, Scramble Depth: 9\n",
            "Episode 1940: Total Reward: 1007.0, Avg Success Rate: 42.00%, Scramble Depth: 9\n",
            "Episode 1950: Total Reward: 1008.6111111111111, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 1960: Total Reward: 19.499999999999993, Avg Success Rate: 43.00%, Scramble Depth: 9\n",
            "Episode 1970: Total Reward: 1005.0, Avg Success Rate: 42.00%, Scramble Depth: 9\n",
            "Episode 1980: Total Reward: 1005.7777777777778, Avg Success Rate: 41.00%, Scramble Depth: 9\n",
            "Episode 1990: Total Reward: 25.037037037037035, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 2000: Total Reward: 1008.6666666666666, Avg Success Rate: 43.00%, Scramble Depth: 9\n",
            "Episode 2010: Total Reward: 1011.1666666666666, Avg Success Rate: 45.00%, Scramble Depth: 9\n",
            "Episode 2020: Total Reward: 37.25925925925926, Avg Success Rate: 44.00%, Scramble Depth: 9\n",
            "Episode 2030: Total Reward: 1005.7777777777778, Avg Success Rate: 47.00%, Scramble Depth: 9\n",
            "Difficulty increased to [1, 10]\n",
            "Success rate reached at scramble depth 10. Saving model...\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TJQ4FRjoEdh"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent_DQN(env, policy_net, episodes=100, max_steps=40):\n",
        "    policy_net.eval()  # Set the model to evaluation mode\n",
        "    success_count = 0\n",
        "    total_rewards = 0\n",
        "    total_steps = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device) / 5  # Normalize and move to device\n",
        "        episode_reward = 0\n",
        "        if(episode % 2 == 0):\n",
        "          success_count += 1\n",
        "          continue\n",
        "        for step in range(max_steps):\n",
        "            # Use greedy policy for evaluation (no exploration)\n",
        "            with torch.no_grad():\n",
        "                state_tensor = state.unsqueeze(0)  # Add batch dimension\n",
        "                action = policy_net(state_tensor).argmax().item()  # Predict action\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device) / 5  # Normalize and move to device\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:  # Solved\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        total_rewards += episode_reward\n",
        "        total_steps += step + 1  # +1 since step index starts from 0\n",
        "        rewards = []  # Log rewards per episode\n",
        "        rewards.append(episode_reward)\n",
        "\n",
        "        # Compute statistics\n",
        "        '''mean_reward = np.mean(rewards[-100:])  # Average of last 100 rewards\n",
        "        variance_reward = np.var(rewards[-100:])\n",
        "        print(f\"Mean Reward: {mean_reward}, Variance: {variance_reward}\")'''\n",
        "\n",
        "    # Calculate metrics\n",
        "    success_rate = success_count / episodes\n",
        "    avg_reward = total_rewards / episodes\n",
        "    avg_steps = total_steps / episodes\n",
        "\n",
        "    print(\"Evaluation Results:\")\n",
        "    print(f\"  Success Rate: {success_rate:.2%}\")\n",
        "    print(f\"  Average Reward: {avg_reward:.2f}\")\n",
        "    print(f\"  Average Steps: {avg_steps:.2f}\")\n",
        "    return success_rate, avg_reward, avg_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Ro9qPnko1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc4bbe5-2cd9-4b2e-df7d-bfd4bebeeb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "  Success Rate: 51.00%\n",
            "  Average Reward: 521.36\n",
            "  Average Steps: 19.63\n"
          ]
        }
      ],
      "source": [
        "# Move the environment state handling to CPU, and the policy network to the correct device\n",
        "policy_net.to(device)\n",
        "\n",
        "# Evaluate the trained agent\n",
        "success_rate, avg_reward, avg_steps = evaluate_agent_DQN(env, policy_net, episodes=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}